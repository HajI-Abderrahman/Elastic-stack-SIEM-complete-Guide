# Elastic-stack-SIEM-complete-Guide
# Introdction and support mention

this is a detialed Elastic stack SIEM complete Guide, wich is a project that i have work on it 
*{ to work on it}*

# How can you benifit from this reposotry:

for take the best from this reposetory you can flow the order of modules and section wich will start by explaining the fandemantels of SIEM on genarel then, you will find a explanation of Eastic stack his componates and how it‚Äôs working, then we will explain step by how you can deploye a ELK SIEM (for practice we will call elastick stack =only‚áí ‚ÄúELK‚Äù the old name of the product‚Äôs wich means the basic components of it‚Äôs wich they are Elasticsearch, Kibana, Logstash),
then will working on deffirente use cases with and deffirentes system sources.

if you want to get more deep undrstanding of the product you can merge between the officiel documentation and this reposetory.
So, Give your Cap of coffe and let‚Äôs start on.

---

# Lab ressources :

Essiantials cmoponents :

- we have work the the version 8.19 of Elastik search .
- the first version of the Lab we will install al the compneants using  VMs on vmware workstation 17.
- i have using Ubuntu 22.04 for install the compnanants on it.

system sources that i have use :

- Windows server 2019
- ubuntu  server 22.04
- Fortigate FW
- cisco SWs *{Upcomming}*
- Tripwire *{Upcomming}*
- Wallix *{Upcomming}*
- other linux distrupution *{Upcomming}*
- palo alto FW *{Upcomming}*
- WAF *{Upcomming}*
- cloud Azur *{Upcomming}*
- PGP (symantic encryption) *{Upcomming}*

---

# Architecture propos√©e et explications

*this will be metionned under te section of instalation Steps
and there you could also mention that we will work on an other version where the architucture will focus on how to deploye a cluster* 

Tu veux :

- **3 machines** pour un **cluster Elasticsearch** (noeuds)
- **1 machine** pour Kibana
- **1 machine** pour Logstash + Fleet Server

Ensuite, pour les sources :

- 1 machines **Windows** qui envoient via **Elastic Agent ‚Üí Fleet Server ‚Üí Elasticsearch**
- **Tripwire** qui envoie ses logs via **Logstash pipeline** vers Elasticsearch.

utilises Elastic Agent + ingestion par Logstash.

## phase 2 : configue √©tape par √©tape

## üõ†Ô∏è √âtapes pour installer ton cluster ‚Äútol√©rance + performance raisonnable‚Äù (5 VMs)

### ‚úÖ Pr√©paration avant l‚Äôinstallation

1. **Configurer les VMs dans VMware**
    - [ ]  Cr√©e 5 VMs Ubuntu Server (version recommand√©e : 20.04 ou 22.04).
    - [ ]  Assure-toi que chaque VM peut acc√©der aux autres via le r√©seau NAT `192.168.12.0`.
    - [ ]  Attribue des adresses IP statiques et hostname par exemple :
    
    - es-node-1 : `192.168.12.20`
    - es-node-2 : `192.168.12.21`
    - es-node-3 : `192.168.12.22`
    - kibana : `192.168.12.23`
    - Logstash-Fleet : `192.168.12.24`
    
    ```bash
    sudo hostnamectl set-hostname <hostname>
    nano /etc/hosts
    # en l align o il y a 127.0.1.1 met le vouveaux hostname
    sudo reboot
    ```
    
    exampl 
    
    ![image.png](attachment:b57247a0-7b4c-44f3-858e-28d7469ae6c0:image.png)
    
    installation et activation du  service ssh et 
    
    ```bash
    sudo apt update
    sudo apt install openssh-server
    sudo systemctl¬†enable¬†ssh
    ```
    
    etend√© la dur√© du session
    
    ```bash
    sudo apt update && sudo apt upgrade -y
    ```
    
    1. d√©finition d‚Äôune @ IP statique en 
        
        vous allez entrer au dossier netplan au vous allez trouvez le configuration r√©seaux de la machine, o vous devez d√©sactivez DHCP, et d√©finir une @ ip statique:
        
        ```jsx
        sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg
        network: {config: disabled}
        sudo rm /etc/netplan/50-cloud-init.yaml
        nano /etc/netplan/01-cloud-init.yaml
        ```
        
        ```yaml
        network:
          version: 2
          renderer: networkd # Or 'systemd-networkd'
          ethernets:
            ens33: # <-- Replace with your interface name
              dhcp4: no
              addresses:
                - 192.168.12.10/24 # <-- Your desired Static IP/CIDR
              routes:
                - to: default
                  via: 192.168.12.2 # <-- Your Gateway IP
              nameservers:
                addresses: [8.8.8.8, 1.1.1.1] # <-- DNS Servers (e.g., Google/Cloudflare)
        ```
        
        ```jsx
        cat /etc/netplan/50-cloud-init.yaml
        sudo netplan appl
        ```
        

prendre une snapshot

1. **Configurer `vm.max_map_count` sur chaque n≈ìud Elasticsearch**
2. **Installer Java (si n√©cessaire)**
    
    Selon la version d‚ÄôElasticsearch, Java peut √™tre requis ou non. Si besoin :
    
    ```bash
    sudo apt update
    sudo apt install openjdk-17-jdk -y
    java -version
    ```
    
    # üëáüëáüëá
    
    üóíÔ∏è cloner les vm et adapter chaqu‚Äôun √† la configue n√©ssaicaire hostnames et ip
    
    ```bash
    sudo hostnamectl set-hostname <hostname>
    nano /etc/hosts
    # en l align o il y a 127.0.1.1 met le vouveaux hostname
    sudo reboot
    ```
    
    - Elasticsearch exige une valeur √©lev√©e pour `vm.max_map_count`. Ex√©cute sur **chaque** VM ES :
        
        ```bash
        sudo sysctl -w vm.max_map_count=262144
        # Pour le rendre permanent :
        echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
        sudo sysctl -p
        
        ```
        
        - reason üí°
            
            
            ÿ™ÿ®ÿßÿ±ŸÉ ÿßŸÑŸÑŸá ÿπŸÑŸäŸÉ! ŸÜÿπŸÖÿå ÿßŸÑÿ£ŸàÿßŸÖÿ± ÿßŸÑÿ™Ÿä ÿ¥ÿ±ÿ≠ÿ™Ÿáÿß:
            
            Bash
            
            # 
            
            `sudo sysctl -w vm.max_map_count=262144
            # ... ŸÑÿ¨ÿπŸÑŸáÿß ÿØÿßÿ¶ŸÖÿ©`
            
            **ÿ∂ÿ±Ÿàÿ±Ÿäÿ© ÿ¨ÿØÿßŸã** ŸÑÿ•ŸÜÿ¨ÿßÿ≠ Ÿàÿ™ÿ£ŸÖŸäŸÜ ÿ£ÿØÿßÿ° **Cluster ÿØŸäÿßŸÑ Elasticsearch** (ŸàŸáŸà ÿßŸÑŸÖŸÉŸàŸÜ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ŸÅŸä ELK Stack).
            
            ---
            
            ## üõë ŸÑŸÖÿßÿ∞ÿß Ÿáÿ∞ÿß ÿßŸÑÿ•ÿπÿØÿßÿØ ÿ∂ÿ±Ÿàÿ±Ÿä ŸÑŸÑŸÄ Clusterÿü
            
            ÿßŸÑÿ≥ÿ®ÿ® Ÿäÿ±ÿ¨ÿπ ÿ•ŸÑŸâ ŸÉŸäŸÅŸäÿ© ÿπŸÖŸÑ ŸÖÿ≠ÿ±ŸÉ ÿßŸÑÿ®ÿ≠ÿ´ **Lucene** (ÿßŸÑÿ∞Ÿä Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸäŸá Elasticsearch):
            
            1. **ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© (Memory Mapping):** Elasticsearch Ÿäÿ≥ÿ™ÿÆÿØŸÖ ŸÖŸÉÿ™ÿ®ÿßÿ™ Lucene ÿßŸÑÿ™Ÿä ÿ™ÿπÿ™ŸÖÿØ ÿ®ÿ¥ŸÉŸÑ ŸÖŸÉÿ´ŸÅ ÿπŸÑŸâ ÿ™ŸÇŸÜŸäÿ© ÿ™ÿ≥ŸÖŸâ **Memory-mapped files (mmap)**. Ÿáÿ∞Ÿá ÿßŸÑÿ™ŸÇŸÜŸäÿ© ÿ™ÿ≥ŸÖÿ≠ ŸÑŸÄ Elasticsearch ÿ®ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© (ŸÖÿ´ŸÑ ŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÄ Index) ŸÉŸÖÿß ŸÑŸà ŸÉÿßŸÜÿ™ ÿ¨ÿ≤ÿ°ÿßŸã ŸÖŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©ÿå ŸÖŸÖÿß Ÿäÿ≤ŸäÿØ ÿ®ÿ¥ŸÉŸÑ ŸÉÿ®Ÿäÿ± ŸÖŸÜ ÿ≥ÿ±ÿπÿ© ÿßŸÑÿ®ÿ≠ÿ´ ŸàÿßŸÑÿ£ÿØÿßÿ°.
            2. **ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ ŸÑŸÑŸÖŸÜÿßÿ∑ŸÇ (`vm.max_map_count`):** ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ± `vm.max_map_count` Ÿäÿ≠ÿØÿØ ÿßŸÑÿπÿØÿØ ÿßŸÑÿ£ŸÇÿµŸâ ŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÄ mmap ÿßŸÑÿ™Ÿä ŸäŸÖŸÉŸÜ ŸÑŸÄ Elasticsearch ŸÅÿ™ÿ≠Ÿáÿß ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸàŸÇÿ™.
            3. **ŸÅÿ¥ŸÑ Cluster (ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖŸÜÿÆŸÅÿ∂ÿßŸã):**
                - ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÇŸäŸÖÿ© ÿßŸÑÿßŸÅÿ™ÿ±ÿßÿ∂Ÿäÿ© (ÿπÿßÿØÿ©Ÿã 65530) ÿµÿ∫Ÿäÿ±ÿ© ÿ¨ÿØÿßŸãÿå ŸÅŸÄ Elasticsearch ÿ≥ŸäŸÅÿ¥ŸÑ ŸÅŸä ŸÅÿ™ÿ≠ ŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÄ Index ÿßŸÑŸÑÿßÿ≤ŸÖÿ© ŸÑŸÑÿπŸÖŸÑ.
                - ŸÅŸä ÿ®Ÿäÿ¶ÿ© **Cluster**ÿå ÿ≠Ÿäÿ´ ŸäŸÇŸàŸÖ ŸÉŸÑ **Node** ÿ®ÿ™ÿÆÿ≤ŸäŸÜ Ÿàÿ•ÿØÿßÿ±ÿ© ÿπÿØÿØ ŸÉÿ®Ÿäÿ± ŸÖŸÜ **Shards** Ÿà **Indices**ÿå ŸÅÿ•ŸÜ ÿπÿØÿØ ŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÄ mmap ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© Ÿäÿ™ÿ∂ÿßÿπŸÅ.
                - ÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿ™ŸÖ ÿ±ŸÅÿπ ÿßŸÑŸÇŸäŸÖÿ©ÿå ŸÅÿ•ŸÜ ÿßŸÑŸÄ **Node** ÿ≥Ÿäÿ™ŸàŸÇŸÅ ÿπŸÜ ÿßŸÑÿπŸÖŸÑ ŸÅÿ¨ÿ£ÿ© ÿπŸÜÿØ ÿ™ÿ≠ŸÖŸäŸÑ Shards ÿ¨ÿØŸäÿØÿ© ÿ£Ÿà ÿπŸÜÿØ ÿ®ÿØÿ° ÿßŸÑÿ™ÿ¥ÿ∫ŸäŸÑÿå ŸàŸÇÿØ ŸäÿπŸäŸÇ ÿ•ŸÜÿ¨ÿßÿ≠ ÿπŸÖŸÑŸäÿ© ÿßŸÑŸÄ Clustering ÿ®ÿßŸÑŸÉÿßŸÖŸÑ.
            
            **ÿßŸÑÿÆŸÑÿßÿµÿ©:** ÿ®ÿØŸàŸÜ ÿ±ŸÅÿπ `vm.max_map_count` ÿ•ŸÑŸâ **`262144`** ÿπŸÑŸâ **ÿ¨ŸÖŸäÿπ ÿßŸÑŸÄ Nodes** ŸÅŸä Cluster ÿØŸäÿßŸÑŸÉÿå ÿ≥ÿ™Ÿàÿßÿ¨Ÿá ŸÖÿ¥ÿßŸÉŸÑ ŸÅŸä ÿßÿ≥ÿ™ŸÇÿ±ÿßÿ± ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ŸàÿßŸÑÿ£ÿØÿßÿ°ÿå ŸàŸÇÿØ ÿ™ŸÅÿ¥ŸÑ ÿπŸÖŸÑŸäÿ© ÿßŸÑŸÄ Cluster ŸÅŸä ÿßŸÑÿπŸÖŸÑ ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠.
            
            ---
            
            ## üìù ŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿ£ÿÆÿ±Ÿâ ÿ∂ÿ±Ÿàÿ±Ÿäÿ© ŸÑŸÑŸÄ Cluster
            
            ÿ®ÿßŸÑÿ•ÿ∂ÿßŸÅÿ© ÿ•ŸÑŸâ `vm.max_map_count`ÿå Ÿäÿ¨ÿ® ÿπŸÑŸäŸÉ ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿßŸÑÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿπŸÑŸâ **ŸÉŸÑ Node** ŸÅŸä ÿßŸÑŸÄ Cluster:
            
            1. **ÿ™ÿÆÿµŸäÿµ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© (Heap Size):** Ÿäÿ¨ÿ® ÿ™ÿπŸäŸäŸÜ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿÆÿµÿµÿ© ŸÑŸÄ Java (Heap) ŸÅŸä ŸÖŸÑŸÅ **`jvm.options`** ŸÑÿ™ŸÉŸàŸÜ ŸÜÿµŸÅ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿßŸÑŸÖÿ™ŸàŸÅÿ±ÿ© ÿπŸÑŸâ ÿßŸÑŸÄ VM (ŸÖÿ´ŸÑÿßŸãÿå ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÑÿØŸäŸÉ 16GB RAMÿå Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿπŸäŸÜ `Xms8g` Ÿà `Xmx8g`).
            2. **ÿ™ÿπÿ∑ŸäŸÑ ÿßŸÑŸÄ Swap:** Ÿäÿ¨ÿ® **ÿ™ÿπÿ∑ŸäŸÑ ÿßŸÑŸÄ Swap (ŸÖŸÑŸÅ ÿßŸÑÿ™ÿ®ÿßÿØŸÑ)** ŸÑŸÖŸÜÿπ ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ™ÿ¥ÿ∫ŸäŸÑ ŸÖŸÜ ŸÜŸÇŸÑ ÿßŸÑŸÄ Heap ÿØŸäÿßŸÑ Elasticsearch ÿ•ŸÑŸâ ÿßŸÑŸÇÿ±ÿµÿå ŸÖŸÖÿß ŸäÿØŸÖÿ± ÿßŸÑÿ£ÿØÿßÿ°.
                - (ŸäŸÖŸÉŸÜ ÿ™ÿπÿ∑ŸäŸÑŸáÿß ÿπÿ®ÿ± ÿßŸÑÿ£ŸÖÿ± `sudo swapoff -a` Ÿàÿ•ŸÑÿ∫ÿßÿ° ÿ™ÿπŸÑŸäŸÇ ÿ≥ÿ∑ÿ± ÿßŸÑŸÄ swap ŸÅŸä `/etc/fstab`).
            3. **ÿßŸÑŸÄ File Descriptors:** ÿ±ŸÅÿπ ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ ŸÑÿπÿØÿØ ÿßŸÑŸÄ File Descriptors ÿßŸÑŸÖŸÅÿ™Ÿàÿ≠ÿ© ÿ•ŸÑŸâ ŸÇŸäŸÖÿ© ÿπÿßŸÑŸäÿ© (ŸÖÿ´ŸÑÿßŸã 65536) ŸÅŸä ŸÖŸÑŸÅ `limits.conf`.

---

## 1Ô∏è‚É£ Installation des n≈ìuds Elasticsearch (3 VMs)

Fais cette partie sur chacune des 3 VMs Elasticsearch (Node1, Node2, Node3).

1. **Ajouter le d√©p√¥t Elasticsearch**
    
    ```bash
    wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
    sudo apt-get install apt-transport-https
    echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
    sudo apt update
    
    ```
    
2. **Installer Elasticsearch**
    
    ```bash
    sudo apt install elasticsearch -y
    
    ```
    
3. **Activer et d√©marrer Elasticsearch**
    
    ```bash
    sudo /bin/systemctl daemon-reload
    sudo /bin/systemctl enable elasticsearch.service
    sudo systemctl start elasticsearch.service
    sudo systemctl status elasticsearch.service
    ```
    
    then change the password by : `/usr/share/elasticsearch/bin/elasticsearch-reset-password -i -u elastic`
    
    ‚áí elkcluster
    
    vivrifier si elasticsearch work by : `curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic https://localhost:9200`
    
4. **Configurer `elasticsearch.yml`**
    
    make copy of the yml file 
    
    `cp /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/elasticsearch.yml.backup`
    
    √âdite `/etc/elasticsearch/elasticsearch.yml` sur chaque n≈ìud. Par exemple :
    
    - **Node1 (`192.168.12.20`)** :
        
        ```yaml
        #--------------------first step------------------------
        # modify only this lignes and left all by default intill the next step
        node.name: es-node-1
        network.host: 192.168.12.20 #pourquelle √™tre accissible avec cette @ et pas seulement en localhost
        http.port: 9200
        ```
        
        allow port 9200 on FW on tcp & udp then reload.
        
        ```bash
        sudo ufw allow 9200/tcp
        sudo ufw allow 9200/udp
        sudo ufw status
        # 1. ÿ™ÿπÿ∑ŸäŸÑ (ŸÑÿ≠ÿ∏Ÿä)
        sudo ufw disable
        
        # 2. ÿ™ŸÅÿπŸäŸÑ ŸÖÿ¨ÿØÿØÿßŸã (ŸÖÿπ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑÿ¨ÿØŸäÿØÿ©)
        sudo ufw enable
        ```
        
        ```yaml
        # ------------------next step -----------------------
        cluster.name: mon-cluster
        transport.port: 9300
        
        discovery.seed_hosts: ["192.168.12.20", "192.168.12.21", "192.168.12.22"]
        cluster.initial_master_nodes: ["es-node-1", "es-node-2", "es-node-3"]
        
        node.roles: [ "master", "data" ]
        
        ```
        
    - **Node2 (`192.168.12.11`)** :
        
        ```yaml
        cluster.name: mon-cluster
        node.name: es-node-2
        network.host: 192.168.12.21
        http.port: 9200
        transport.port: 9300
        
        discovery.seed_hosts: ["192.168.12.20", "192.168.12.21", "192.168.12.22"]
        # ne pas r√©p√©ter initial_master_nodes si tu as d√©j√† d√©marr√© tout le cluster, mais pour le bootstrap initial c‚Äôest ok
        cluster.initial_master_nodes: ["es-node-1", "es-node-2", "es-node-3"]
        
        node.roles: [ "master", "data" ]
        
        ```
        
    - **Node3 (`192.168.12.12`)** :
        
        ```yaml
        cluster.name: mon-cluster
        node.name: es-node-3
        network.host: 192.168.12.22
        http.port: 9200
        transport.port: 9300
        
        discovery.seed_hosts: ["192.168.12.20", "192.168.12.21", "192.168.12.22"]
        cluster.initial_master_nodes: ["es-node-1", "es-node-2", "es-node-3"]
        
        node.roles: [ "master", "data" ]
        
        ```
        
    
    > Cette configuration met 3 n≈ìuds master-eligible + data, ce qui est recommand√© pour un petit cluster avec tol√©rance aux pannes. (Discuss the Elastic Stack)
    > 
5. **V√©rifier le cluster**
    
    Depuis l‚Äôun des n≈ìuds (ou un client) :
    
    ```bash
    curl http://192.168.12.20:9200/_cluster/health?pretty
    curl http://192.168.12.20:9200/_cat/nodes?v
    
    ```
    

---

## 2Ô∏è‚É£ Installation de **Kibana** (1 VM)

Sur la VM Kibana (`192.168.12.23`).

1. **Ajouter le d√©p√¥t Kibana**
    
    ```jsx
    wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
    sudo apt-get install apt-transport-https
    echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
    ```
    
    ```bash
    sudo apt update
    sudo apt install kibana -y
    sudo /bin/systemctl daemon-reload
    sudo /bin/systemctl enable kibana.service
    sudo systemctl start kibana.service
    sudo systemctl status kibana.service
    ```
    
2. **Configurer `kibana.yml`**
    
    Ouvre `/etc/kibana/kibana.yml` et mets :
    
    but befor taking an backup by  : 
    
    `cp /etc/kibana/kibana.yml /etc/kibana/kibana.yml.backup`
    
    the  `nano /etc/kibana/kibana.yml`
    
    ```yaml
    #------------------------first step-------------------
    server.port: 5601
    server.host: "192.168.12.23"
    #=====================system: elasticsearch====================
    elasticsearch.hosts: ["https://192.168.12.20: 9200"]
    elasticsearch.username: "kibana_system"
    elasticsearch.password: "elkcluster"
    #======================system: elasticsearch(opt)
    elasticsearch.ssl.certificateAuthorities: [ "/etc/kibana/certs/http_ca.crt"]
    ```
    
    where going to copy the certificatesfrom elasticsearch into kibana because we are going to use https for communication.
    
    cr√©e le dosier premierement en kibana : 
    
    - `mkdir /etc/kibana/certs/`
    
    du la machine node 1 
    
    ```bash
    # 1. ŸÜÿ≥ÿÆ ÿßŸÑŸÖŸÑŸÅ ÿ•ŸÑŸâ ŸÖÿ¨ŸÑÿØ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿπÿßÿØŸä haji ŸÅŸä ÿßŸÑÿÆÿßÿØŸÖ 192.168.12.23
    scp /etc/elasticsearch/certs/http_ca.crt haji@192.168.12.23:/home/haji/
    
    # 2. ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑÿØÿÆŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿÆÿßÿØŸÖ ÿßŸÑŸáÿØŸÅ ŸàŸÜŸÇŸÑ ÿßŸÑŸÖŸÑŸÅ
    ssh haji@192.168.12.23
    # (ÿØÿßÿÆŸÑ ÿßŸÑÿÆÿßÿØŸÖ 192.168.12.23)
    sudo mv /home/haji/http_ca.crt /etc/kibana/certs/
    ```
    
    changer  le mode pass pr√©built in  du kibana parce que on vas autoriser d‚Äôauthtifier par mots de pass au lieux du token enrollement par : 
    
    `/usr/share/elasticsearch/bin/elasticsearch-reset-password -i -u kibana_system`  ‚áí √† : elkcluster
    
    en la machine kibana
    
    vous pouver verfifer avec  `cat`  si le fichier √† √©t√© bien transmet et que le fichier yml de configuration et bien enregisterer
    
     and then autoriser le port 5601 en tcp et aussi udp par parfeu
    
    ```bash
    sudo ufw allow 5601/tcp
    sudo ufw status
    # 1. ÿ™ÿπÿ∑ŸäŸÑ (ŸÑÿ≠ÿ∏Ÿä)
    sudo ufw disable
    
    # 2. ÿ™ŸÅÿπŸäŸÑ ŸÖÿ¨ÿØÿØÿßŸã (ŸÖÿπ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑÿ¨ÿØŸäÿØÿ©)
    sudo ufw enable
    ```
    
    ```bash
    sudo systemctl restart kibana.service
    sudo systemctl status kibana.service
    ```
    
    after you get into the kibana interface you see wornning notification pup up you will fixe it by set a public address for kibana in the yml file 
    
    ```yaml
    server.publicBaseUrl: "http://192.168.12.23:5601"
    ```
    
    red√©marer le systme aprer de sauvgarder les modification que vous fait en le fichier yml
    
3. **Acc√©der √† Kibana**
    1. **Acc√©der √† Kibana**
        
        Dans ton navigateur ‚Üí `http://192.168.12.13:5601`
        
        puis s‚Äôauthantifier par user name : `elastic` et password: `elkcluster`
        
    

```yaml
#------------------------next step-------------------
elasticsearch.hosts: ["http://192.168.12.10:9200","http://192.168.12.11:9200","http://192.168.12.12:9200"]

```

---

## 3Ô∏è‚É£ Installation de **Logstash + Fleet Server** (1 VM)

Sur ta VM d√©di√©e (par exemple `192.168.12.14`).

1. **Installer Logstash**
    
    ```bash
    wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elastic-keyring.gpg
    sudo apt-get install apt-transport-https
    echo "deb [signed-by=/usr/share/keyrings/elastic-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list
    sudo apt update
    sudo apt install logstash -y
    ```
    
2. **Installer Elastic Agent / Fleet Server**
    - T√©l√©charge l‚ÄôElastic Agent (version compatible avec ton Elasticsearch).
    - Suis la documentation de Fleet Server pour l‚Äôinstaller comme serveur : tu devras configurer `fleet.yml` pour pointer vers ton cluster ES, g√©n√©rer des certificats si tu veux TLS, etc. (voir docs Elastic : Fleet Server)
    - tester avec WD server et ses int√©gration et down one by one of the nodes to test all of them , surtout la node 1
    
    üí°üö® 
    
    - pour bien monitorer elasticsearch i faut installler :
        - [ ]  il faut installer un agent en elasticsearchs nodes (policiers du Debians) pour bien monitorer les nodes d‚Äôune manier avanc√© (essayer de trouver  üëâüëçune m√©thode de ignorer certification ‚ÄºÔ∏è‚ÄºÔ∏ècomme celle utiliser en  windows issues ou si non on vas √™tres besoin cr√©e une certification du fleet server  (commme celle cr√©e d√©jat cr√©e par  ES http_ca.crt) ‚ùì‚ùì)
        - [ ]  ajouter l‚Äôint√©grartoin du elasticsearch √† la policier de sans agent.
        
        chek l‚Äô√©tat de l‚Äôint√©gration tous doit √©tres en vers
        
3. **Configurer un pipeline Logstash**
    
    prend du snapsohte ou vms ‚áí puis tester avec ce que j‚Äôai du fortigate ‚áírevenir ver les derniers snapshots ‚áí (et test  fortigat avec la m√©thode du fleet ) ‚áídemader du fatima le fichier du config si ms coll√©gues e ont pas le trouvez
    
    Cr√©e un fichier de pipeline, par exemple `/etc/logstash/conf.d/agent-pipeline.conf` :
    
    ```
    # ==============================================================================
    # 1. INPUT STAGE: Receive Logs from FortiGate
    # ==============================================================================
    input {
      udp {
        host => "192.168.12.10"
        port => 5144            # FortiGate Syslog Port
    #    codec => plain
      }
    }
    
    # ==============================================================================
    # 2. FILTER STAGE: Parse and Process FortiGate Logs
    # ==============================================================================
    filter {
      grok {
        match => {"message" => "%{SYSLOG5424PRI}%{GREEDYDATA:message}" }
        overwrite => [ "message" ]
      }
    
      mutate {
        remove_field => ["@timestamp", "path", "host", "@version", "log", "event"]
      }
    
      kv {
        field_split => " "
      }
      mutate {
      remove_field => ["message"]
      add_field => { "logdate" => "%{date} %{time}"}
      }
    
      date {
        match => [ "logdate", "yyyy-MM-dd HH:mm:ss" ]
    #    timezone => "America/edmonton"
        target => "@timestamp"
      }
      mutate {
        remove_field => ["logdate", "date", "time"] # ‚úÖ CORRECT SYNTAX
        convert => { "rcvdbyte" => "integer" }
        convert => { "sentbyte" => "integer" }
     }
    }
    # ==============================================================================
    # 3. OUTPUT STAGE: Send Logs to Elasticsearch
    # ==============================================================================
    output {
      stdout {
        codec => rubydebug
      }
      elasticsearch {
        hosts => ["https://192.168.12.10"]
        index => "firewall-%{+YYYY.MM.dd}"
        user => "elastic"
        password => "FP+eESzDMGixNOd95ra7"
        ssl => true
        cacert => "/etc/logstash/http_ca.crt"
      }
    
    }
    ```
    
4. **D√©marrer Logstash**
    
    ```bash
    sudo systemctl enable logstash --now
    sudo systemctl status logstash
    
    ```
    
5. **Configurer Fleet Server dans l‚ÄôElastic Agent**
    - Lors de l‚Äô‚Äúenrollment‚Äù des agents, indique l‚Äôadresse de ton Fleet Server (`192.168.12.14`).
    - V√©rifie que les agents s‚Äôenregistrent et envoient des logs.

---

## 4Ô∏è‚É£ Test & validation de ton cluster

1. **Valider le cluster ES**
    - Fais `curl _cluster/health` ‚Üí doit √™tre ‚Äúgreen‚Äù ou ‚Äúyellow‚Äù (avec 3 n≈ìuds data + master).
    - Regarde `_cat/nodes?v` pour v√©rifier que les 3 n≈ìuds sont bien dans le cluster.
2. **Valider Logstash ingestion**
    - Configure un agent Elastic sur ta machine Windows ou Linux test.
    - Envoie quelques logs ‚Üí v√©rifier dans Kibana (index `agent-logs-*`).
    - V√©rifie dans Logstash les logs de pipeline (erreurs, parsing‚Ä¶).
3. **Valider Kibana**
    - Cr√©e un ‚ÄúIndex Pattern‚Äù dans Kibana / ‚ÄúData View‚Äù pour les index cr√©√©s par Logstash.
    - Va dans Discover pour voir les documents.
    - Cr√©e des visualisations simples (par ex. nombre de logs par type) pour v√©rifier que tout marche.
4. **Test de tol√©rance**
    - Arr√™te un des n≈ìuds Elasticsearch (par exemple ES-Node3) :
        
        ```bash
        sudo systemctl stop elasticsearch
        
        ```
        
    - V√©rifie avec `/_cluster/health` que le cluster reste en vie (quorum).
    - Red√©marre le n≈ìud et assure-toi qu‚Äôil r√©int√®gre le cluster sans probl√®me.

---

## 5Ô∏è‚É£ Conseils & meilleures pratiques

- **Surveille les ressources** : Assure-toi que chaque VM a assez de RAM. Si les n≈ìuds ES sont limit√©s, ajuste les `Xms` / `Xmx` dans `/etc/elasticsearch/jvm.options`.
- **Backups** : M√™me dans un lab, fais des snapshots de ta VM ou des backups de tes donn√©es ES si tu veux garder des things importantes.
- **Logs & Monitoring** : Active la journalisation d‚ÄôElasticsearch (`/var/log/elasticsearch`) et surveille les erreurs.
- **S√©curit√©** : Si tu veux rendre ton cluster plus s√©curis√©, active TLS entre n≈ìuds (`xpack.security.transport.ssl.*`) et l‚Äôauthentification.
- **Nettoyage** : Si tu r√©initialises souvent, n‚Äôoublie pas de vider le dossier `data` (`/var/lib/elasticsearch/`) avant de red√©marrer un n≈ìud pour √©viter des conflits de cluster. Certains ont eu des erreurs quand ils ont r√©utilis√© des donn√©es sans nettoyage. ([Reddit](https://www.reddit.com/r/elasticsearch/comments/yx7m4f?utm_source=chatgpt.com))

---

Si tu veux, je peux te donner **un script bash automatis√©** que tu peux ex√©cuter sur chaque VM Elasticsearch pour configurer ton cluster automatiquement (avec tes IP). Tu veux que je le pr√©pare ?